{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model overfitting\n",
    "* This simple notebook demonstrates how to overfit a linear regression\n",
    "* It really isn't that hard\n",
    "* Also, it shows basic problem for linear regresssion in the \"big data\" world\n",
    "* It then tries to fix this using Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load helpers\n",
    "# Will try to just load what I need on this\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate linear data experiments\n",
    "def genLinData(N,M,noise):\n",
    "    # y = x_1 + x_2 .. x_M + eps\n",
    "    # X's scaled so the variance of explained part is same order as noise variance (if std(eps) = 1)\n",
    "    sigNoise = np.sqrt(1./M)\n",
    "    X = np.random.normal(size=(N,M),loc=0,scale=sigNoise)\n",
    "    eps = np.random.normal(size=N,loc=0,scale=noise)\n",
    "    y = np.sum(X,axis=1)+eps\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model equation:\n",
    "* $y = \\sum_{i=1}^M x_i + \\epsilon_i$\n",
    "* $E(y)=E(x_i) = E(\\epsilon)=0$\n",
    "* $\\hat y =  \\sum_{i=1}^M x_i$\n",
    "* $\\text{Var}({x_i}) = (1/M)$\n",
    "* $\\text{Var}({\\sum x_i}) = 1 = \\text{Var} (\\hat y)$\n",
    "* Equals $\\epsilon$ or noise variance, if noise = 1\n",
    "* Variance of y = $\\text{Var}(\\hat y) + \\text{Var}(\\epsilon)$\n",
    "* Variance of y = 1 + 1 = 2\n",
    "* R-squared = $1-\\frac{(N-1)*\\text{Var}(\\epsilon)}{(N-1)\\text{Var}(y)}=( 1 - \\frac{1}{2}) = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594622173261824\n",
      "0.49184435234549695\n"
     ]
    }
   ],
   "source": [
    "# Set up a system that you might have estimated once in your econometrics class\n",
    "X, y = genLinData(200,2,1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=0)\n",
    "# Now run regression\n",
    "# print score, which is R-squared (fit)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now increase right hand side forecast variables (a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7221767771647211\n",
      "-0.09568655969851038\n"
     ]
    }
   ],
   "source": [
    "# Set up a system you would be told never to try in your econometrics class\n",
    "X, y = genLinData(200,50,1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=0)\n",
    "# Now run regression\n",
    "# print score, which is R-squared (fit)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick monte-carlo example\n",
    "* Note: you can do this many times\n",
    "* This is known as a monte-carlo\n",
    "* See code below\n",
    "* There is a big for loop \n",
    "* Statistics for each run are stored in numpy vector (scoreVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06395555193541601\n",
      "0.24810739882055938\n",
      "0.57\n"
     ]
    }
   ],
   "source": [
    "scoreVec = np.zeros(100)\n",
    "for i in range(100):\n",
    "    X, y = genLinData(200,50,1.0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5)\n",
    "    # Now run regression\n",
    "    # print score, which is R-squared (fit)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    scoreVec[i] = lr.score(X_test,y_test)\n",
    "print(np.mean(scoreVec))\n",
    "print(np.std(scoreVec))\n",
    "print(np.mean(scoreVec<0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase sample size\n",
    "* The ultimate solution for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5053594300644131\n",
      "0.5006110188740948\n"
     ]
    }
   ],
   "source": [
    "# Set up a system you would be told never to try in your econometrics class\n",
    "X, y = genLinData(20000,50,1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=0)\n",
    "# Now run regression\n",
    "# print score, which is R-squared (fit)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "* This regression is clearly overfitting\n",
    "* Try many different runs of this\n",
    "* This is a form of overfitting\n",
    "* **Note:**  The model is technically the correct model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Ridge and Lasso\n",
    "* Control overfitting\n",
    "* Parameter alpha now added\n",
    "* Note: In this experiment no coefficients are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6644539932566441\n",
      "0.21595608677509104\n"
     ]
    }
   ],
   "source": [
    "X, y = genLinData(200,50,1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=0)\n",
    "# Now run regression\n",
    "# print score, which is R-squared (fit)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(ridge.score(X_train,y_train))\n",
    "print(ridge.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7495972396450203\n",
      "0.21305567850556162\n"
     ]
    }
   ],
   "source": [
    "X, y = genLinData(200,50,1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=0)\n",
    "# Now run regression\n",
    "# print score, which is R-squared (fit)\n",
    "lasso = Lasso(alpha=0.0025)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(lasso.score(X_train,y_train))\n",
    "print(lasso.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more complicated model\n",
    "* Set some of the parameters to zero\n",
    "* This may give even more overfitting\n",
    "* Can also see how well Lasso gets zeros\n",
    "* Finding irrelevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate linear data experiments\n",
    "# Now drop some coefficients to zero (sparse)\n",
    "def genLinData(N,M,noise):\n",
    "    # y = x_1 + x_2 .. x_M + eps\n",
    "    # X's scaled so the variance of explained part is same order as noise variance (if eps = 1)\n",
    "    sigNoise = np.sqrt(1./M)\n",
    "    # set up random beta for regression\n",
    "    beta = np.random.normal(size=(M,1),loc=0.,scale=1.)\n",
    "    # force smaller beta to zero\n",
    "    beta[abs(beta)<1.0] = 0.\n",
    "    betaUsed= np.sum( beta != 0.)\n",
    "    X = np.random.normal(size=(N,M),loc=0,scale=sigNoise)\n",
    "    eps = np.random.normal(size=(N,1),loc=0,scale=noise)\n",
    "    # Modern Python with matrix multiplication\n",
    "    y = X @ beta + eps\n",
    "    # Find theoretical best R-squared \n",
    "    sse = np.sum(eps**2)\n",
    "    meany = np.mean(y)\n",
    "    sse2 = np.sum( (y-meany)**2)\n",
    "    trsquared = 1. - sse/sse2\n",
    "    # Old style Python\n",
    "    # X = np.random.normal(size=(N,M),loc=0,scale=sigNoise)\n",
    "    # eps = np.random.normal(size=N,loc=0,scale=noise)\n",
    "    # y = np.sum(X,axis=1)+eps\n",
    "    return X,y,betaUsed,trsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3623177643098451\n",
      "0.6630147842772923\n",
      "-0.2614681852302312\n",
      "11\n",
      "[[-0.234874    0.03971918  0.32634017 -0.59338448 -3.11108987  0.80041525\n",
      "   0.18174424  2.72293106  1.61032184  0.26993116  1.97945008 -0.70972809\n",
      "  -3.48440867  1.04625833  1.91764536  0.99481565  0.35054434 -1.90481734\n",
      "  -0.57162909  1.99496871  1.52291838  1.22144749  0.94243827 -1.21353232\n",
      "   1.60256293  0.47984155  0.42780426  0.62576777  1.38493944  0.91529516\n",
      "   0.52462835  0.71160237 -0.78313485 -0.15311965  1.9133465  -0.41988404\n",
      "  -0.37285608 -1.18004611  0.06855708 -0.17456601  0.80243753  0.36359459\n",
      "   0.22190386  0.42904851 -0.82950563 -1.60486922  2.13449515  0.35694226\n",
      "  -0.72197241 -0.21780494]]\n",
      "0.983317592177224\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "X, y, nvar, trs = genLinData(200,50,1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=0)\n",
    "# Now run regression\n",
    "# print score, which is R-squared (fit)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(trs)\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))\n",
    "print(nvar)\n",
    "print(lr.coef_)\n",
    "print(np.mean(np.abs(lr.coef_)))\n",
    "print(np.sum(lr.coef_!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now quick monte-carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43552205285165335\n",
      "0.662334029841268\n",
      "0.03047442731798991\n",
      "0.393\n"
     ]
    }
   ],
   "source": [
    "nmc = 1000\n",
    "scoreVec = np.zeros(nmc)\n",
    "scoreVecInSamp = np.zeros(nmc)\n",
    "trsVec = np.zeros(nmc)\n",
    "for i in range(nmc):\n",
    "    X, y, nvar, trs = genLinData(250,50,1.0)\n",
    "    trsVec[i] = trs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5)\n",
    "    # Now run regression\n",
    "    # print score, which is R-squared (fit)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    scoreVecInSamp[i] = lr.score(X_train,y_train)\n",
    "    scoreVec[i] = lr.score(X_test,y_test)\n",
    "print(np.mean(trsVec))\n",
    "print(np.mean(scoreVecInSamp))\n",
    "print(np.mean(scoreVec))\n",
    "print(np.mean(scoreVec<0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5538727291581442\n",
      "0.6703858582383189\n",
      "0.26977902601379067\n",
      "16\n",
      "[[-0.55840901  0.59240777 -0.44913178 -0.72686675  0.42131789  1.27170035\n",
      "  -0.51080305 -1.15142713 -0.36628985  0.03354132  0.10587337  0.08297305\n",
      "  -0.28566255  0.62801971  0.35810673  0.13903004  1.13413042 -1.61236532\n",
      "  -0.11968789  0.61950813  0.42234329 -0.3411994   0.44469962 -0.60867638\n",
      "   0.23748332 -0.03132906  0.62059148 -0.32642761  0.41894926  0.43588149\n",
      "   1.59077311 -0.2058528  -1.6363259  -0.50350675  0.5995654  -0.69209899\n",
      "   0.93543383 -0.54547788  0.37274492 -1.26079047  0.00425337  0.33390983\n",
      "   1.3838307   0.01298133 -0.87311486  0.56586691  1.37627098  0.15299158\n",
      "  -0.11912182 -0.13459007]]\n",
      "0.5670866901044388\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "X, y, nvar, trs = genLinData(200,50,1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=0)\n",
    "# Now run regression\n",
    "# print score, which is R-squared (fit)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(trs)\n",
    "print(ridge.score(X_train,y_train))\n",
    "print(ridge.score(X_test,y_test))\n",
    "print(nvar)\n",
    "print(ridge.coef_)\n",
    "print(np.mean(np.abs(ridge.coef_)))\n",
    "print(np.sum(ridge.coef_!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-carlo for Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42908497283829583\n",
      "0.5909055202869715\n",
      "0.23836805352482052\n",
      "0.016\n"
     ]
    }
   ],
   "source": [
    "nmc = 1000\n",
    "scoreVec = np.zeros(nmc)\n",
    "scoreVecInSamp = np.zeros(nmc)\n",
    "trsVec = np.zeros(nmc)\n",
    "for i in range(nmc):\n",
    "    X, y, nvar, trs = genLinData(250,50,1.0)\n",
    "    trsVec[i] = trs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5)\n",
    "    # Now run regression\n",
    "    # print score, which is R-squared (fit)\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    scoreVecInSamp[i] = ridge.score(X_train,y_train)\n",
    "    scoreVec[i] = ridge.score(X_test,y_test)\n",
    "print(np.mean(trsVec))\n",
    "print(np.mean(scoreVecInSamp))\n",
    "print(np.mean(scoreVec))\n",
    "print(np.mean(scoreVec<0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35662063350190165\n",
      "0.28350409208513905\n",
      "0.1656704596944899\n",
      "13\n",
      "[ 0.         -0.          0.          0.         -0.52623042  0.\n",
      "  0.          0.         -0.          0.48457794 -0.61271486  0.\n",
      "  0.         -0.          0.82655622 -1.21371223 -0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.\n",
      "  0.         -0.         -0.          0.         -0.75886404  0.\n",
      "  0.         -0.          0.31681958 -1.10036995 -0.         -0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.         -0.          0.          0.         -0.73493616 -0.\n",
      "  0.04827762 -0.        ]\n",
      "0.13246118018198014\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "X, y, nvar, trs = genLinData(200,50,1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=0)\n",
    "# Now run regression\n",
    "# print score, which is R-squared (fit)\n",
    "lasso = Lasso(alpha=0.025)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(trs)\n",
    "print(lasso.score(X_train,y_train))\n",
    "print(lasso.score(X_test,y_test))\n",
    "print(nvar)\n",
    "print(lasso.coef_)\n",
    "print(np.mean(np.abs(lasso.coef_)))\n",
    "print(np.sum(lasso.coef_!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-carlo for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4331689925224723\n",
      "0.30920184876847406\n",
      "0.15945682588307072\n",
      "0.023\n"
     ]
    }
   ],
   "source": [
    "nmc = 1000\n",
    "scoreVec = np.zeros(nmc)\n",
    "scoreVecInSamp = np.zeros(nmc)\n",
    "trsVec = np.zeros(nmc)\n",
    "for i in range(nmc):\n",
    "    X, y, nvar, trs = genLinData(250,50,1.0)\n",
    "    trsVec[i] = trs\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5)\n",
    "    # Now run regression\n",
    "    # print score, which is R-squared (fit)\n",
    "    lasso = Lasso(alpha=0.025)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    scoreVecInSamp[i] = lasso.score(X_train,y_train)\n",
    "    scoreVec[i] = lasso.score(X_test,y_test)\n",
    "print(np.mean(trsVec))\n",
    "print(np.mean(scoreVecInSamp))\n",
    "print(np.mean(scoreVec))\n",
    "print(np.mean(scoreVec<0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
